{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Learning Modeling Tasks - Reducing Catastrophic Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Overview and Context\n",
    "\n",
    "Continual learning in LLMs aims to enable these models to learn new tasks and adapt to new data without forgetting previously learned information. This project addresses the challenge of catastrophic forgetting by enhancing GPT models with continual learning capabilities. This advancement has significant potential applications in automated customer service and dynamic content creation.\n",
    "\n",
    "Goal\n",
    "\n",
    "To explore methods for enabling LLMs to continually learn and adapt to new data or tasks without forgetting previously learned information, thereby addressing catastrophic forgetting.\n",
    "\n",
    "Objectives\n",
    "1. Mitigate Catastrophic Forgetting: Implement and test Elastic Weight Consolidation (EWC) on GPT-2.\n",
    "2. Adapt GPT for Continual Learning: Integrate continual learning mechanisms within the Transformer architecture.\n",
    "3. Evaluate Model Performance: Use backward and forward transfer metrics to measure performance on old vs. new tasks.\n",
    "4. Understand Transformer Architecture: Explore the self-attention mechanisms and their scalability in transformers.\n",
    "\n",
    "## Dataset Description\n",
    "Dataset: WikiText-103\n",
    "• Description: A collection of over 100 million tokens from verified Good and Featured articles on Wikipedia.\n",
    "• Usage: To test the model’s ability to learn continually and adapt over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Elastic Weight Consolidation (EWC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Setup**: Load GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "\n",
    "# Select a smaller subset of the dataset\n",
    "small_dataset = dataset['train'].select(range(50)) \n",
    "\n",
    "# Convert the dataset to DataLoader\n",
    "train_dataloader = DataLoader(small_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fisher Information Matrix Calculation:** calculating the Fisher Information Matrix (FIM) is an essential part of the Elastic Weight Consolidation (EWC) method used to mitigate catastrophic forgetting in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fisher Information Matrix Calculation\n",
    "def compute_fisher_information(model, dataloader):\n",
    "    model.eval()\n",
    "    fisher_information = {n: torch.zeros(p.shape).to(p.device) for n, p in model.named_parameters() if p.requires_grad}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        targets = inputs['input_ids'].clone()\n",
    "        \n",
    "        if targets.numel() == 0:  # Check for empty sequences\n",
    "            continue\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits.view(-1, model.config.vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                fisher_information[n] += p.grad.pow(2)\n",
    "    \n",
    "    for n in fisher_information:\n",
    "        fisher_information[n] /= len(dataloader)\n",
    "    \n",
    "    return fisher_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **EWC Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EWC Regularization Term in the Loss Function\n",
    "class EWC:\n",
    "    def __init__(self, model, dataloader, lambda_=0.4):\n",
    "        self.model = model\n",
    "        self.lambda_ = lambda_\n",
    "        self.fisher_information = compute_fisher_information(model, dataloader)\n",
    "        self.optimal_params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                _loss = self.fisher_information[n] * (p - self.optimal_params[n]).pow(2)\n",
    "                loss += _loss.sum()\n",
    "        return (self.lambda_ / 2) * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function for old task\n",
    "def evaluate_on_old_task(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            targets = inputs['input_ids'].clone()\n",
    "            if targets.numel() == 0:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits.view(-1, model.config.vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Loss on Old Task: {avg_loss}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss on Old Task: 45.74768932049091\n"
     ]
    }
   ],
   "source": [
    "# Evaluate initial performance on the old task\n",
    "initial_loss_old_task = evaluate_on_old_task(model, train_dataloader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any optimizer and EWC training on the model, the average loss is relatively high (45.75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on new task without EWC\n",
    "def train_without_ewc(model, dataloader, optimizer, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            targets = inputs['input_ids'].clone()\n",
    "            if targets.numel() == 0:\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits.view(-1, model.config.vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer for training without EWC\n",
    "optimizer_without_ewc = optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 36.986759185791016\n",
      "Epoch 1, Loss: 34.90500259399414\n",
      "Epoch 1, Loss: 32.675880432128906\n",
      "Epoch 1, Loss: 27.8104305267334\n",
      "Epoch 1, Loss: 14.99581241607666\n",
      "Epoch 1, Loss: 17.95258331298828\n",
      "Epoch 1, Loss: 9.375332832336426\n",
      "Epoch 1, Loss: 11.026851654052734\n",
      "Epoch 1, Loss: 6.626292705535889\n",
      "Epoch 1, Loss: 2.9959139823913574\n",
      "Epoch 1, Loss: 4.661403656005859\n",
      "Epoch 1, Loss: 7.900046348571777\n",
      "Epoch 1, Loss: 4.165240287780762\n",
      "Epoch 2, Loss: 6.586829662322998\n",
      "Epoch 2, Loss: 3.544839382171631\n",
      "Epoch 2, Loss: 3.9093141555786133\n",
      "Epoch 2, Loss: 2.3981270790100098\n",
      "Epoch 2, Loss: 4.0951433181762695\n",
      "Epoch 2, Loss: 1.8137054443359375\n",
      "Epoch 2, Loss: 6.375125885009766\n",
      "Epoch 2, Loss: 3.5391931533813477\n",
      "Epoch 2, Loss: 4.002185344696045\n",
      "Epoch 2, Loss: 3.7483954429626465\n",
      "Epoch 2, Loss: 4.445273399353027\n",
      "Epoch 2, Loss: 2.932441473007202\n",
      "Epoch 3, Loss: 3.5660438537597656\n",
      "Epoch 3, Loss: 3.4430465698242188\n",
      "Epoch 3, Loss: 1.8920410871505737\n",
      "Epoch 3, Loss: 3.4411191940307617\n",
      "Epoch 3, Loss: 2.6871933937072754\n",
      "Epoch 3, Loss: 2.1164700984954834\n",
      "Epoch 3, Loss: 3.16133451461792\n",
      "Epoch 3, Loss: 2.9189486503601074\n",
      "Epoch 3, Loss: 3.884322166442871\n",
      "Epoch 3, Loss: 2.6519203186035156\n",
      "Epoch 3, Loss: 1.6760612726211548\n",
      "Epoch 3, Loss: 1.6092190742492676\n",
      "Epoch 3, Loss: 3.288991928100586\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the new task\n",
    "train_without_ewc(model, train_dataloader, optimizer_without_ewc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss on Old Task: 2.7562116292806773\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance on the old task after training on the new task without EWC\n",
    "loss_after_training_without_ewc = evaluate_on_old_task(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the optimizer for training the model, the average loss decreased rapidly (2.76)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training**: Implement training with EWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with EWC\n",
    "def train_with_ewc(model, train_dataloader, ewc, optimizer, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            targets = inputs['input_ids'].clone()\n",
    "            \n",
    "            if targets.numel() == 0:  # Check for empty sequences\n",
    "                continue\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits.view(-1, model.config.vocab_size), targets.view(-1))\n",
    "            ewc_loss = ewc.penalty(model)\n",
    "            total_loss = loss + ewc_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            targets = inputs['input_ids'].clone()\n",
    "            \n",
    "            if targets.numel() == 0:  # Check for empty sequences\n",
    "                continue\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits.view(-1, model.config.vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Average Loss: {avg_loss}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.975815534591675\n",
      "Epoch 1, Loss: 3.889411687850952\n",
      "Epoch 1, Loss: 1.4942246675491333\n",
      "Epoch 1, Loss: 2.014646291732788\n",
      "Epoch 1, Loss: 1.7066130638122559\n",
      "Epoch 1, Loss: 1.3164194822311401\n",
      "Epoch 1, Loss: 0.9031688570976257\n",
      "Epoch 1, Loss: 1.8186219930648804\n",
      "Epoch 1, Loss: 0.8964985013008118\n",
      "Epoch 1, Loss: 0.6037851572036743\n",
      "Epoch 1, Loss: 0.6174091696739197\n",
      "Epoch 1, Loss: 0.29234981536865234\n",
      "Epoch 1, Loss: 0.806888222694397\n",
      "Epoch 2, Loss: 1.5767492055892944\n",
      "Epoch 2, Loss: 0.39598917961120605\n",
      "Epoch 2, Loss: 0.516277551651001\n",
      "Epoch 2, Loss: 0.3632330894470215\n",
      "Epoch 2, Loss: 0.6245399117469788\n",
      "Epoch 2, Loss: 0.25948378443717957\n",
      "Epoch 2, Loss: 0.18636707961559296\n",
      "Epoch 2, Loss: 0.4856903553009033\n",
      "Epoch 2, Loss: 0.19470864534378052\n",
      "Epoch 2, Loss: 0.22150105237960815\n",
      "Epoch 2, Loss: 0.9134004712104797\n",
      "Epoch 2, Loss: 0.34479355812072754\n",
      "Epoch 2, Loss: 0.23868535459041595\n",
      "Epoch 3, Loss: 0.1113562360405922\n",
      "Epoch 3, Loss: 0.37803328037261963\n",
      "Epoch 3, Loss: 0.11955796927213669\n",
      "Epoch 3, Loss: 0.21330222487449646\n",
      "Epoch 3, Loss: 0.10219863802194595\n",
      "Epoch 3, Loss: 0.3395010828971863\n",
      "Epoch 3, Loss: 0.1815841943025589\n",
      "Epoch 3, Loss: 0.10912475734949112\n",
      "Epoch 3, Loss: 0.201659694314003\n",
      "Epoch 3, Loss: 0.22853286564350128\n",
      "Epoch 3, Loss: 0.13934536278247833\n",
      "Epoch 3, Loss: 0.3519052267074585\n",
      "Epoch 3, Loss: 0.25839242339134216\n",
      "Average Loss: 0.11365048300761443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11365048300761443"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize EWC\n",
    "ewc = EWC(model, train_dataloader)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train the model with EWC\n",
    "train_with_ewc(model, train_dataloader, ewc, optimizer)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using EWC, the performance is better, the average loss becomes 0.113, however, the training time is 8 minutes 50 secondes, on 50 sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Progressive Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the progressive prompt class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressivePrompt(nn.Module):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super(ProgressivePrompt, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_embeddings = nn.ParameterList()\n",
    "    \n",
    "    def add_prompt(self, new_prompt):\n",
    "        prompt_ids = self.tokenizer(new_prompt, return_tensors='pt').input_ids\n",
    "        prompt_embeddings = self.model.transformer.wte(prompt_ids)\n",
    "        self.prompt_embeddings.append(nn.Parameter(prompt_embeddings.squeeze(0)))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size = input_ids.size(0)\n",
    "        prompt_embeds = torch.cat([prompt_embed.unsqueeze(0) for prompt_embed in self.prompt_embeddings], dim=1)\n",
    "        prompt_embeds = prompt_embeds.expand(batch_size, -1, -1)\n",
    "        inputs_embeds = self.model.transformer.wte(input_ids)\n",
    "        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)\n",
    "        labels = torch.cat([torch.full((batch_size, prompt_embeds.size(1)), -100).to(input_ids.device), input_ids], dim=1)\n",
    "        return self.model(inputs_embeds=inputs_embeds, labels=labels)\n",
    "\n",
    "    def get_combined_prompt_text(self):\n",
    "        return \" \".join([self.tokenizer.decode(prompt_embed.detach().cpu().numpy()) for prompt_embed in self.prompt_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune the model with trainable prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_with_progressive_prompts(model, tokenizer, data_loader, progressive_prompt, new_prompt_text, epochs=3, lr=5e-5):\n",
    "    # Add the new prompt to the model\n",
    "    progressive_prompt.add_prompt(new_prompt_text)\n",
    "    \n",
    "    optimizer = AdamW(progressive_prompt.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in data_loader:\n",
    "            inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "            inputs = inputs.to(model.device).long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = progressive_prompt(inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.239480972290039\n",
      "Epoch 1, Loss: 8.5606689453125\n",
      "Epoch 1, Loss: 8.844741821289062\n",
      "Epoch 1, Loss: 9.052119255065918\n",
      "Epoch 1, Loss: 6.741456508636475\n",
      "Epoch 1, Loss: 13.632518768310547\n",
      "Epoch 1, Loss: 3.760716199874878\n",
      "Epoch 1, Loss: 6.807703495025635\n",
      "Epoch 1, Loss: 8.914515495300293\n",
      "Epoch 1, Loss: 6.7754364013671875\n",
      "Epoch 1, Loss: 7.414560317993164\n",
      "Epoch 1, Loss: 5.2529168128967285\n",
      "Epoch 1, Loss: 21.523788452148438\n",
      "Epoch 2, Loss: 4.346595287322998\n",
      "Epoch 2, Loss: 5.344362258911133\n",
      "Epoch 2, Loss: 17.516157150268555\n",
      "Epoch 2, Loss: 10.0841646194458\n",
      "Epoch 2, Loss: 8.682435989379883\n",
      "Epoch 2, Loss: 8.15819263458252\n",
      "Epoch 2, Loss: 3.232306480407715\n",
      "Epoch 2, Loss: 5.076308727264404\n",
      "Epoch 2, Loss: 7.424879550933838\n",
      "Epoch 2, Loss: 9.098407745361328\n",
      "Epoch 2, Loss: 25.525550842285156\n",
      "Epoch 2, Loss: 13.476286888122559\n",
      "Epoch 2, Loss: 20.564903259277344\n",
      "Epoch 3, Loss: 12.324183464050293\n",
      "Epoch 3, Loss: 5.501623153686523\n",
      "Epoch 3, Loss: 12.944634437561035\n",
      "Epoch 3, Loss: 17.254173278808594\n",
      "Epoch 3, Loss: 8.060469627380371\n",
      "Epoch 3, Loss: 8.099797248840332\n",
      "Epoch 3, Loss: 7.161810874938965\n",
      "Epoch 3, Loss: 12.557515144348145\n",
      "Epoch 3, Loss: 12.567479133605957\n",
      "Epoch 3, Loss: 4.976041316986084\n",
      "Epoch 3, Loss: 4.138189792633057\n",
      "Epoch 3, Loss: 8.087166786193848\n",
      "Epoch 3, Loss: 6.840652942657471\n",
      "Average Loss: 12.935417175292969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.935417175292969"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize progressive prompts\n",
    "progressive_prompt = ProgressivePrompt(model, tokenizer)\n",
    "\n",
    "# Fine-tune the model on the new task\n",
    "new_prompt_text = \"This is a new task prompt.\"\n",
    "model = fine_tune_with_progressive_prompts(model, tokenizer, train_dataloader, progressive_prompt, new_prompt_text)\n",
    "\n",
    "# Evaluate the model\n",
    "test_data = [\"Some test sentence for evaluation.\"]\n",
    "evaluate_model(model, tokenizer, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Low-Rank Adaptation (LoRA)\n",
    "- **Model Setup**: Apply LoRA to GPT-2.\n",
    "- **Training**: Code for fine-tuning with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import apply_lora\n",
    "\n",
    "model = apply_lora(model, rank=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for input_text in dataset:\n",
    "        inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- **Metrics**: Define and calculate backward transfer, forward transfer, perplexity, and other metrics.\n",
    "- **Results**: Present results for each method and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, old_task_dataset, new_task_dataset):\n",
    "    # Implement backward transfer, forward transfer, and perplexity calculations\n",
    "    pass\n",
    "\n",
    "old_task_metrics = calculate_metrics(model, old_task_dataset, new_task_dataset)\n",
    "new_task_metrics = calculate_metrics(model, new_task_dataset, old_task_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(old_task_metrics, new_task_metrics):\n",
    "    metrics = ['backward_transfer', 'forward_transfer', 'perplexity']\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(old_task_metrics[metric], label='Old Task')\n",
    "        plt.plot(new_task_metrics[metric], label='New Task')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "        plt.title(f'{metric} over time')\n",
    "        plt.show()\n",
    "\n",
    "plot_results(old_task_metrics, new_task_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
